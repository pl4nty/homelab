apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: longhorn
  namespace: longhorn
spec:
  interval: 30m
  chart:
    spec:
      chart: longhorn
      version: 1.8.1
      sourceRef:
        kind: HelmRepository
        name: longhorn
        namespace: flux-system
  maxHistory: 3
  install:
    createNamespace: true
    remediation:
      retries: 3
    timeout: 15m
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    persistence:
      # defaultClass: false # no option to set param for v2 engine
      defaultClassReplicaCount: 2
      # defaultDataLocality: best-effort
    csi:
      attacherReplicaCount: 1
      provisionerReplicaCount: 1
      resizerReplicaCount: 1
      snapshotterReplicaCount: 1
    defaultSettings:
      backupTarget: s3://tplant-homelab-volumes@us-west-004/
      backupTargetCredentialSecret: longhorn-backup
      allowRecurringJobWhileVolumeDetached: true
      # defaultDataPath: /var/lib/longhorn (file-type, not block-type like /dev )
      createDefaultDiskLabeledNodes: true
      # use this to avoid cross-node volume access - but the scheduler doesn't respect it
      # systemManagedComponentsNodeSelector: node.longhorn.io/create-default-disk:config
      v1DataEngine: true
      v2DataEngine: false
      backupstorePollInterval: 0 # only used by DR volumes not backups, expensive S3 list calls
    # this also applies to engines :(
    # longhornManager:
    #   nodeSelector:
    #     node.longhorn.io/create-default-disk: config
    longhornUI:
      replicas: 1

# k annotate storageclass longhorn storageclass.kubernetes.io/is-default-class=true

# for data v2: "diskType": "block"

# k annotate node <node> node.longhorn.io/default-disks-config='[{
#    "path": "/var/mnt/nvme0n1",
#    "allowScheduling": true
# }]'
# k label node <node> node.longhorn.io/create-default-disk=config
