# yaml-language-server: $schema=https://flux-helm-schema.tplant.com.au?values=https://raw.githubusercontent.com/grafana/k8s-monitoring-helm/refs/heads/main/charts/k8s-monitoring/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8s-monitoring
  namespace: monitoring-system
spec:
  interval: 15m
  chart:
    spec:
      chart: k8s-monitoring
      version: 2.0.4
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
  maxHistory: 3
  install:
    createNamespace: true
    remediation:
      retries: 3
    crds: CreateReplace
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
    crds: CreateReplace
  uninstall:
    keepHistory: false
  values:
    cluster:
      name: ${CLUSTER_NAME:=bootstrap}
    destinations:
      - name: metrics
        type: prometheus
        auth:
          type: basic
        secret:
          create: false
          name: k8s-monitoring-metrics
          namespace: monitoring-system
        metricProcessingRules: |-
          rule {
            source_labels = ["namespace","pod"]
            separator = "@"
            regex = "flux-system@(.+)-.+-.+"
            target_label = "uid"
            replacement = ""
          }
          rule {
            source_labels = ["namespace","pod"]
            separator = "@"
            regex = "flux-system@(.+)-.+-.+"
            target_label = "pod"
            replacement = "$1"
          }
      - name: logs
        type: loki
        auth:
          type: basic
        secret:
          create: false
          name: k8s-monitoring-logs
          namespace: monitoring-system
      - name: traces
        type: otlp
        auth:
          type: basic
        secret:
          create: false
          name: k8s-monitoring-traces
          namespace: monitoring-system

    clusterMetrics:
      enabled: true
    clusterEvents:
      enabled: true
    nodeLogs:
      enabled: true
    podLogs:
      enabled: true
    # high metrics
    # applicationObservability:
    # eBPF, but we already have OTel
    # autoInstrumentation:
    # just use prom CRDs
    # annotationAutodiscovery:
    prometheusOperatorObjects:
      enabled: true
    # 4GB/day but only 50GB/month limit
    # profiling:
    # likely high metrics
    # integrations:
    selfReporting:
      enabled: false

    alloy-metrics:
      enabled: true
    alloy-logs:
      enabled: true
    alloy-singleton:
      enabled: true

    # logs:
    #   extraConfig: |-
    #     loki.source.syslog "syslog_udp" {
    #       listener {
    #         address = "0.0.0.0:514"
    #         protocol = "udp"
    #         labels = { job = "integrations/syslog_exporter" }
    #       }

    #       forward_to = [loki.write.grafana_cloud_loki.receiver]
    #     }
    # grafana-agent-logs:
    #   agent:
    #     extraPorts:
    #       - name: syslog-udp
    #         port: 514
    #         targetPort: 514
    #         protocol: UDP
    # metrics:
    #   kube-state-metrics:
    #     metricsTuning:
    #       excludeMetrics:
    #         - kube_replicaset_status_.*
    #         - kube_replicaset_created
    #         - kube_replicaset_metadata_generation
    #         - kube_replicaset_owner
    #         - kube_replicaset_spec_replicas
    #         - kube_pod_status_reason
    #         - node_namespace_pod_container:container_memory_cache
    #         - node_namespace_pod_container:container_memory_swap
    #         - kube_pod_container_status_restarts_total
    #         - kubelet_cgroup_manager_duration_seconds_bucket
    #         - kubelet_pod_start_duration_seconds_bucket
    #         - container_fs_.*
    #         - cluster:namespace:pod_.*
    #         - container_network_.*
    #   node-exporter:
    #     metricsTuning:
    #       dropMetricsForFilesystem:
    #         - tempfs # default
    #         - tmpfs
